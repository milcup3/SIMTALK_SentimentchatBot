# main.py
import argparse
import os
import sys
import random
from pathlib import Path
from typing import List

import numpy as np
import pandas as pd
from openai import OpenAI

from app.retriever import SimpleIndex
from app.chatbot import answer_naive, answer_multiquery, answer_hyde
from app.config import load_env, assert_env  # assert_envê°€ ì—†ë‹¤ë©´ load_envë§Œ ì‚¬ìš©í•˜ì„¸ìš”
from app.question_manager import QuestionManager
from app.ranker import EmotionKeywordAnalyzer
from app.report_generator import generate_final_report
from app.prompting import (
    NEXT_QUESTION_SYSTEM,
    build_next_question_user_prompt,
    REFINE_QUESTION_SYSTEM,
    build_refine_question_user_prompt,
)


#===========ê²½ê³  ë¬¸êµ¬,ë””ë²„ê¹… ë¬¸êµ¬ ì œê±°===========
import warnings, logging
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")
try:
    from transformers.utils import logging as hf_logging
    hf_logging.set_verbosity_error()
except Exception:
    pass
# ============ ìƒìˆ˜/í—¬í¼ ============
PROMPT_PREFIX = "ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ë‹¤ìŒ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"


def resolve_sct_path(user_path: str, root: Path) -> Path:
    """SCT ì§ˆë¬¸ í…œí”Œë¦¿ ê²½ë¡œ ì•ˆì „í™”"""
    p = Path(user_path)
    cands = [p if p.is_absolute() else (root / p)]
    # í”í•œ ìœ„ì¹˜ í›„ë³´
    cands += [
        root / "sct_questions.jsonl",
        root.parent / "sct_questions.jsonl",
    ]
    for c in cands:
        if c.exists():
            #print(f"[INFO] SCT í…œí”Œë¦¿ ì‚¬ìš©: {c}")
            return c
    raise FileNotFoundError(
        "SCT ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:\n  " + "\n  ".join(str(x) for x in cands)
    )


def resolve_analysis_csv(user_path: str, root: Path) -> Path:
    """ë¦¬í¬íŠ¸ìš© ë¶„ì„ CSV ê²½ë¡œ ìë™ íƒìƒ‰"""
    p = Path(user_path)
    cands = [p if p.is_absolute() else (root / p)]
    cands += [
        root / "Data" / "train.csv",
        root / "Data" / "train_data.csv",
        root.parent / "Data" / "train.csv",
        root.parent / "Data" / "train_data.csv",
    ]
    for c in cands:
        if c.exists():
            #print(f"[INFO] ë¶„ì„ CSV ì‚¬ìš©: {c}")
            return c
    raise FileNotFoundError(
        "ë¶„ì„ìš© CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:\n  " + "\n  ".join(str(x) for x in cands)
    )


def build_next_question(client: OpenAI, gen_model: str, history: List[dict], fallback_pool: List[str]) -> str:
    """ì§ì „ 2í„´ ë¬¸ë§¥ì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒ ì§ˆë¬¸ 1ë¬¸ì¥ ìƒì„±"""
    ctx = "".join(
        f"ìƒë‹´ê°€: {conv['question']}\nì‚¬ìš©ì: {conv['answer']}\n"
        for conv in history[-2:]
    )
    prompt = f"{PROMPT_PREFIX}{ctx}\nìƒë‹´ê°€:"
    resp = client.chat.completions.create(
        model=gen_model,
        messages=[
            {"role": "system", "content": "í•œêµ­ì–´ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.7,
        max_tokens=100,
    )
    q = (resp.choices[0].message.content or "").strip()
    return q if q else random.choice(fallback_pool)


# ============ ë¦¬í¬íŠ¸ìš© ê²½ëŸ‰ RAG (íŒŒì¼ ë‚´ í¬í•¨) ============
class _MiniDoc:
    def __init__(self, text: str):
        self.page_content = text

class MiniRAG:
    """Response í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ê°„ë‹¨ ìœ ì‚¬ë„ ê²€ìƒ‰í•˜ëŠ” ê²½ëŸ‰ RAG"""
    def __init__(self, texts: List[str], client: OpenAI, embed_model: str):
        self.texts = texts
        self.client = client
        self.embed_model = embed_model
        self.vecs = self._embed(texts)  # (N, D) L2 ì •ê·œí™”

    def _embed(self, texts: List[str]) -> np.ndarray:
        BATCH = 100  # í•œë²ˆì— 100ê°œì”©
        arrs = []
        for i in range(0, len(texts), BATCH):
            chunk = texts[i:i+BATCH]
            embs = self.client.embeddings.create(model=self.embed_model, input=chunk).data
            arrs.append(np.array([e.embedding for e in embs], dtype="float32"))
        arr = np.vstack(arrs)
        arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-8)
        return arr

    def similarity_search(self, query: str, k: int = 1) -> List[_MiniDoc]:
        q = self._embed([query])[0]
        sims = self.vecs @ q
        top = sims.argsort()[-k:][::-1]
        return [_MiniDoc(self.texts[i]) for i in top]


# ============ ë©”ì¸ ============
def main():
    # ---- ì¸ì
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["naive", "multiquery", "hyde", "sct"], default="naive")
    ap.add_argument("--mq", type=int, default=4, help="multiquery ê°œìˆ˜")
    ap.add_argument("--sct_questions", type=str, default="../sct_questions.jsonl", help="SCT ì§ˆë¬¸ í…œí”Œë¦¿ ê²½ë¡œ")
    ap.add_argument("--analysis_csv", type=str, default="../Data/train.csv", help="ë¶„ì„ìš© RAG csv ê²½ë¡œ")
    args = ap.parse_args()

    # ---- ë£¨íŠ¸/ê²½ë¡œ
    ROOT = Path(__file__).resolve().parent
    sct_path = resolve_sct_path(args.sct_questions, ROOT)
    analysis_csv_path = resolve_analysis_csv(args.analysis_csv, ROOT)

    # ---- í™˜ê²½ & í´ë¼ì´ì–¸íŠ¸
    env = load_env()
    # assert_env()ê°€ ìˆë‹¤ë©´ í˜¸ì¶œ (ì—†ë‹¤ë©´ load_env ê²°ê³¼ë§Œ ì‚¬ìš©)
    try:
        assert_env()
    except Exception:
        pass

    openai_api_key = env["OPENAI_API_KEY"]
    embed_model = env["EMBED_MODEL"]
    gen_model = env["GEN_MODEL"]
    client = OpenAI(api_key=openai_api_key)

    # ---- ê¸°ë³¸ ì¸ë±ìŠ¤ (ëŒ€í™” RAG)
    index = SimpleIndex("index")  # index/vectors.npz, meta.json í•„ìš”

    # ===================== SCT ëª¨ë“œ =====================
    if args.mode == "sct":
        print("ğŸ§  SCT 5í„´ ëŒ€í™” ë° ë¦¬í¬íŠ¸ ëª¨ë“œ")

        # 1) ì§ˆë¬¸ í…œí”Œë¦¿ ë¡œë“œ
        qm = QuestionManager(str(sct_path))

        # 2) ì‚¬ìš©ì ì •ë³´
        print("=" * 50)
        print("ğŸ¤– ìƒë‹´ì„ ì‹œì‘í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ì •ë³´ë¥¼ ì—¬ì­¤ë³¼ê²Œìš”.")
        print("=" * 50)
        user_info = {
            "name": input("ğŸ‘¤ ì´ë¦„: "),
            "age": input("ğŸ‘¤ ë‚˜ì´: "),
            "gender": input("ğŸ‘¤ ì„±ë³„: "),
        }

        # 3) ì¹´í…Œê³ ë¦¬ ì„ íƒ
        cats = qm.get_categories()
        print("\n" + "=" * 50)
        print("ğŸ’¬ ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")
        print("=" * 50) 
        for i, cat in enumerate(cats):
            print(f"  [{i+1}] {cat}")
        while True:
            try:
                choice = int(input("\nğŸ‘¤ ì›í•˜ëŠ” ì£¼ì œì˜ ë²ˆí˜¸ ì„ íƒ: "))
                if 1 <= choice <= len(cats):
                    selected_category = cats[choice - 1]
                    break
                print("âš ï¸ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âš ï¸ ìˆ«ìë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        print(f"\nâœ… '{selected_category}' ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")

        question_pool = qm.get_questions_by_category(selected_category)
        conversation_history = []
        analyzer = EmotionKeywordAnalyzer()
        current_question = random.choice(question_pool)
        seed_q = current_question

        ref = client.chat.completions.create(
            model=gen_model,
            messages=[
                {"role": "system", "content": REFINE_QUESTION_SYSTEM},
                {"role": "user", "content": build_refine_question_user_prompt(seed_q or current_question)},
            ],
            temperature=0.2, max_tokens=60,
        )
        current_question = (ref.choices[0].message.content or current_question).strip()


        # 4) ëŒ€í™”(ìµœëŒ€ 5í„´)
        MAX_TURNS = 5

        # ... (ë£¨í”„]]]]]]

        for turn in range(MAX_TURNS):
            print(f"\nAI ğŸ¤– (ì§ˆë¬¸ {turn+1}/{MAX_TURNS}): {current_question}")
            user_response = input("ë‚˜ ğŸ‘¤: ")

            if user_response.lower() in {"ì¢…ë£Œ", "exit", "quit"}:
                print("ğŸ¤– ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                break

            # ê°ì •/í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë¡ (ê·¸ëŒ€ë¡œ)
            analysis = analyzer.analyze(user_response)
            conversation_history.append({
                "turn": turn + 1,
                "question": current_question,
                "answer": user_response,
                "emotion_label": analysis["emotion_label"],
                "emotion_score": analysis["emotion_score"],
                "keywords": analysis["keywords"],
            })

            # â”€â”€ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (ì§ì „ 2í„´ ë¬¸ë§¥ ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            recent = conversation_history[-2:] if len(conversation_history) >= 2 else conversation_history[-1:]
            turns_text = "".join(
                f"ìƒë‹´ê°€: {c['question']}\nì‚¬ìš©ì: {c['answer']}\n" for c in recent
            )

            # 1) 1ì°¨ ì§ˆë¬¸ ìƒì„±
            gen = client.chat.completions.create(
                model=gen_model,
                messages=[
                    {"role": "system", "content": NEXT_QUESTION_SYSTEM},
                    {"role": "user", "content": build_next_question_user_prompt(turns_text)},
                ],
                temperature=0.3,             # í†¤ ì•ˆì •
                max_tokens=80,
                frequency_penalty=0.2,       # ì¤‘ë³µ ì¤„ì´ê¸°
                presence_penalty=0.0,
            )
            next_q = (gen.choices[0].message.content or "").strip()

            # 2) (ì„ íƒ) í›„í¸ì§‘ìœ¼ë¡œ ë” ë¶€ë“œëŸ½ê²Œ êµì •
            if next_q:
                ref = client.chat.completions.create(
                    model=gen_model,
                    messages=[
                        {"role": "system", "content": REFINE_QUESTION_SYSTEM},
                        {"role": "user", "content": build_refine_question_user_prompt(next_q)},
                    ],
                    temperature=0.2,
                    max_tokens=60,
                )
                next_q = (ref.choices[0].message.content or "").strip()

            # ë¹„ìƒì‹œ fallback
            if not next_q or len(next_q) < 3 or "?" not in next_q:
                next_q = random.choice(question_pool)

            current_question = next_q
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # 5) ë¦¬í¬íŠ¸ìš© RAG ì¤€ë¹„(MiniRAG)
        from mini_text_400 import mini_texts

        df = pd.read_csv(analysis_csv_path)
        if "Response" not in df.columns:
            raise RuntimeError(f"[ì˜¤ë¥˜] CSVì— 'Response' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {analysis_csv_path}")

        responses = df["Response"].dropna().astype(str).tolist()
        responses_small = mini_texts(responses)  # 400ì ë‹¨ìœ„ë¡œ ì••ì¶•

        analysis_rag_db = MiniRAG(responses_small, client, embed_model)
        
        # 6) ë¦¬í¬íŠ¸ ìƒì„±/ì¶œë ¥
        print("\nğŸ“Š ìµœì¢… ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
        final_report = generate_final_report(
            conversation_history=conversation_history,
            user_info=user_info,
            analysis_rag_db=analysis_rag_db,  # similarity_search(text,k) ì§€ì›
            openai_api_key=openai_api_key,
            gen_model=gen_model,
        )

        print("\n[ê¸°ë³¸ ì •ë³´]")
        print(f"  - ì´ë¦„: {user_info['name']}")
        print(f"  - ë‚˜ì´: {user_info['age']}")
        print(f"  - ì„±ë³„: {user_info['gender']}")
        print(f"  - ì„ íƒ ì¹´í…Œê³ ë¦¬: {selected_category}\n")
        print("[ì£¼ìš” í‚¤ì›Œë“œ]")
        print(f"  ì´ë²ˆ ëŒ€í™”ì—ì„œ ìì£¼ ì–¸ê¸‰í•œ í‚¤ì›Œë“œ: {', '.join(final_report['top_keywords'])}\n")
        print("[ê°ì • íŠ¸ë Œë“œ]")
        #print(f"  ê°ì • ì ìˆ˜: {final_report['emotion_trend']}")
        print("[ì‹¬ë¦¬ í•´ì„ ë° ì¡°ì–¸]")
        print(final_report["report_text"])
        return

    # ===================== ê¸°ë³¸ RAG ì±—ë´‡ =====================
    print("ğŸ’¬ OpenAI-only RAG Chatbot")
    print("íƒ€ì´í•‘ì„ ì‹œì‘í•˜ì„¸ìš”. ì¢…ë£Œ: exit")
    while True:
        q = input("\nğŸ™‹ ì§ˆë¬¸: ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        if not q:
            continue
        if args.mode == "naive":
            ans = answer_naive(index, q)
        elif args.mode == "multiquery":
            ans = answer_multiquery(index, q, mq=args.mq)
        else:
            ans = answer_hyde(index, q)
        print("\n" + ans)


if __name__ == "__main__":
    main()
