# report_generator.py (improved)
# - ëŠê¹€ ë°©ì§€: max_tokens ìƒí–¥ + ì´ì–´ì“°ê¸°(pass-2) ë³µêµ¬
# - ìš”ì•½ ê³µì •ì„±: ì „ì²´ 5í„´ ê· í˜• ì••ì¶•(mini_text_400) ìœ ì§€ ê°•í™”
# - ê¸ì • ë§¥ë½ ë³´ì¡´: ê°„ë‹¨í•œ ê¸/ë¶€ í…ìŠ¤íŠ¸ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³´ì •
# - ë§ˆí¬ë‹¤ìš´ ì •ë¦¬: ë¶ˆí•„ìš”í•œ ì¹˜í™˜(r"markdown" â†’ "- ") ì œê±°, í‰ë¬¸ ìœ ì§€
# - ì„¹ì…˜ ì™„ê²°ì„±: 1)~8) ëˆ„ë½ ì‹œ ë³´ì™„ ìƒì„±(pass-3)

from typing import List, Dict, Any
from collections import Counter
import re

from openai import OpenAI
from .config import REPORT_PROMPT_TEMPLATE, SYSTEM_ROLE, assert_env

assert_env()

# ---------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ----------

def strip_markdown(text: str) -> str:
    """UIê°€ ë§ˆí¬ë‹¤ìš´ì„ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ë…ì„± í•´ì¹˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ í‰ë¬¸í™”.
    - ì½”ë“œíœìŠ¤/ì¸ë¼ì¸ì½”ë“œ/í—¤ë” ê¸°í˜¸ ì œê±°
    - ëª©ë¡ ê¸°í˜¸ëŠ” '- 'ë§Œ í‘œì¤€í™” ìœ ì§€
    - ë§í¬ ë¬¸ë²• ì œê±°
    """
    # ì½”ë“œ ë¸”ë¡, ì¸ë¼ì¸ ì½”ë“œ ì œê±°
    text = re.sub(r"```[\s\S]*?```", "", text)
    text = re.sub(r"`([^`]*)`", r"\1", text)
    # í—¤ë” ê¸°í˜¸ ì œê±°('#'ë§Œ ì œê±°, ì œëª© í…ìŠ¤íŠ¸ëŠ” ë‚¨ê¹€)
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)
    # êµµê²Œ/ê¸°ìš¸ì„ ì œê±°
    text = re.sub(r"(\*\*|__)(.*?)\1", r"\2", text)
    text = re.sub(r"(\*|_)(.*?)\1", r"\2", text)
    # ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸ í‘œì¤€í™”
    text = re.sub(r"^\s*[\-\*\+]\s+", "- ", text, flags=re.MULTILINE)
    # ë§í¬ ì œê±° [text](url)
    text = re.sub(r"\[([^\]]+)\]\([^\)]+\)", r"\1", text)
    # ì—¬ë¶„ ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+\n", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text

# ---------- ìš”ì•½ ë° ë§¥ë½ ë³´ì • ----------

def _build_summary_text(conversation_history):
    """
    ğŸ”„ ë³€ê²½ì : ìš”ì•½/ìƒ˜í”Œë§/ì••ì¶• ì—†ì´,
    ëŒ€í™”ì˜ 'ëª¨ë“ ' ì§ˆë¬¸/ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì—°ê²°í•´ ë¦¬í¬íŠ¸ì— ë„£ìŠµë‹ˆë‹¤.
    """
    if not conversation_history:
        return ""
    return "\n".join(
        f"Q: {c.get('question','')}\nA: {c.get('answer','')}"
        for c in conversation_history
    )

_POS_TOKENS = ["ê¸ì •", "ì¢‹", "ì¡´ê²½", "ë°°ìš°", "ì¹œì ˆ", "ê¸°ì¨", "í–‰ë³µ", "ê°ì‚¬", "ì˜ìš•", "ì—´ì‹¬"]
_NEG_TOKENS = ["ì‹¤ë§", "ìš°ìš¸", "ë¶ˆì•ˆ", "í˜ë“¤", "ì‹«", "í™”", "ë¶„ë…¸", "ë‘ë µ", "ê³µí¬", "ì§œì¦"]

def _estimate_positive_ratio(conversation_history: List[Dict[str, Any]]) -> float:
    pos = neg = 0
    for c in conversation_history:
        a = (c.get("answer") or "").lower()
        # í•œê¸€ í¬í•¨ì´ë¯€ë¡œ ë‹¨ìˆœ í¬í•¨ ì²´í¬(ëŒ€ì†Œë¬¸ì í˜¼ìš© ëŒ€ë¹„)
        pos += sum(tok in a for tok in _POS_TOKENS)
        neg += sum(tok in a for tok in _NEG_TOKENS)
        # KoELECTRA ë¼ë²¨ ì‚¬ìš© ê°€ëŠ¥ ì‹œ ë³´ì¡° ì‹ í˜¸
        lbl = (c.get("emotion_label") or "").lower()
        if lbl in {"positive", "ê¸ì •"}: pos += 1
        if lbl in {"negative", "ë¶€ì •"}: neg += 1
    denom = max(1, pos + neg)
    return pos / denom

# ---------- í›„í¸ì§‘(ë ˆì´ì•„ì›ƒ/ë¬¸ì²´) ----------

def _refine_layout_markdown(client: OpenAI, model: str, text: str) -> str:
    prompt = f"""
ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì„ìƒ ë³´ê³ ì„œì˜ í˜•ì‹ê³¼ íë¦„ì„ ìœ ì§€í•˜ë©´ì„œ
- H2 í—¤ë”©(##) ìœ ì§€, ì„¹ì…˜ ê°„ ê³µë°± 1ì¤„,
- ë¬¸ë‹¨ë‹¹ 2~3ë¬¸ì¥,
- ëª©ë¡ì€ '- 'ë¡œ ì •ë¦¬,
- ì¤‘ë³µ/êµ°ë”ë”ê¸° ì œê±°
í•˜ë„ë¡ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}
"""
    res = client.chat.completions.create(
        model=model,
        messages=[{"role":"user","content":prompt}],
        temperature=0.2, max_tokens=2000
    )
    return (res.choices[0].message.content or "").strip()

def _refine_style_coherence(client: OpenAI, model: str, text: str) -> str:
    prompt = f"""
ì•„ë˜ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ê³  ë¬¸ì¥ ì—°ê²°ì„ ë§¤ë„ëŸ½ê²Œ êµì •í•´ ì£¼ì„¸ìš”.
- ë‹¨ì • ëŒ€ì‹  'ë³´ì„/ì‹œì‚¬ë¨/ê°€ëŠ¥ì„±' í‘œí˜„ ì‚¬ìš©
- ê³¼í•œ ê³µì†ì²´/ì¥í™©í•¨ ì¤„ì´ê¸°
- ì˜¤íƒˆì/ë°˜ë³µ ì œê±°

í…ìŠ¤íŠ¸:
{text}
"""
    res = client.chat.completions.create(
        model=model,
        messages=[{"role":"user","content":prompt}],
        temperature=0.2, max_tokens=700
    )
    return (res.choices[0].message.content or "").strip()

# ---------- ì„¹ì…˜ ì™„ê²°ì„± ë³´ì¥ ----------

_SECTION_PATTERNS = [
    r"##\s*1\)\s*ìš”ì•½",
    r"##\s*2\)\s*ê´€ì°°",
    r"##\s*3\)\s*ìœ ì‚¬ ì‚¬ë¡€",
    r"##\s*4\)\s*ê°œì¸ì  ê°•ì /ë³´í˜¸ìš”ì¸",
    r"##\s*5\)\s*ì„ìƒì  ê³µì‹í™”",
    r"##\s*6\)\s*ê¶Œê³ ",
    r"##\s*7\)\s*ì£¼ì˜ ì‹ í˜¸ ë° ëŒ€ì‘",
    r"##\s*8\)\s*ì°¸ê³  ë° í•œê³„",
]

def _missing_sections(md_text: str) -> List[int]:
    missing = []
    for i, pat in enumerate(_SECTION_PATTERNS, start=1):
        if not re.search(pat, md_text):
            missing.append(i)
    return missing

def _complete_missing_sections(client: OpenAI, model: str, draft_md: str) -> str:
    missing = _missing_sections(draft_md)
    if not missing:
        return draft_md
    want = ", ".join(f"{i})" for i in missing)
    prompt = f"""
ì•„ë˜ ë³´ê³ ì„œ ì´ˆì•ˆì—ì„œ {want} ì„¹ì…˜ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.
ë™ì¼í•œ í†¤ê³¼ êµ¬ì¡°ë¡œ í•´ë‹¹ ì„¹ì…˜ì„ ë³´ì™„í•˜ì—¬ ì „ì²´ ë¬¸ì„œë¥¼ ì™„ê²°ì„± ìˆê²Œ ë‹¤ì‹œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
ëª¨ë“  ì„¹ì…˜ ì œëª©ì€ '## n) ì œëª©' í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”.

ì´ˆì•ˆ:
{draft_md}
"""
    res = client.chat.completions.create(
        model=model,
        messages=[{"role":"user","content":prompt}],
        temperature=0.3, max_tokens=900
    )
    return (res.choices[0].message.content or draft_md).strip()

# ---------- ë©”ì¸ ì—”íŠ¸ë¦¬ ----------

def generate_final_report(
    conversation_history: List[Dict[str, Any]],
    user_info: Dict[str, str],
    analysis_rag_db,  # .similarity_search(text, k) ì§€ì›
    openai_api_key: str,
    gen_model: str
) -> Dict[str, Any]:
    client = OpenAI(api_key=openai_api_key)

    # 0) ê· í˜• ìš”ì•½
    summary_text = _build_summary_text(conversation_history)

    # 1) ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
    retrieved_docs = analysis_rag_db.similarity_search(summary_text, k=1)
    retrieved_case = retrieved_docs[0].page_content if retrieved_docs else "(ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ)"

    # 2) í‚¤ì›Œë“œ/ê°ì • ìˆ˜ì§‘
    all_keywords = []
    for conv in conversation_history:
        all_keywords.extend(conv.get('keywords', []))
    top_keywords = [kw for kw, _ in Counter(all_keywords).most_common(5)]
    emotion_scores = [float(conv.get('emotion_score', 0.0)) for conv in conversation_history]

    # 3) ê¸ì • ë§¥ë½ ë³´ì • íŒíŠ¸
    pos_ratio = _estimate_positive_ratio(conversation_history)
    context_hint = """
[ë§¥ë½ íŒíŠ¸]
- ë³¸ ëŒ€í™”ëŠ” ì „ë°˜ì ìœ¼ë¡œ ê¸ì •/ê±´ì„¤ì  ì •ì„œê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë³‘ë¦¬ì  í•´ì„ì„ í”¼í•˜ê³ , ê°•ì ê³¼ ë³´í˜¸ìš”ì¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”.
""" if pos_ratio >= 0.6 else ""

    # 4) 1ì°¨ ìƒì„± (ë„‰ë„‰í•œ í† í°)
    prompt = REPORT_PROMPT_TEMPLATE.format(
        name=user_info.get('name',''),
        age=user_info.get('age',''),
        gender=user_info.get('gender',''),
        summary_text=summary_text,
        retrieved_case=retrieved_case
    )
    prompt = context_hint + "\n" + prompt if context_hint else prompt

    draft = client.chat.completions.create(
        model=gen_model,
        messages=[
            {"role":"system","content": SYSTEM_ROLE or "í•œêµ­ì–´ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€"},
            {"role":"user","content": prompt},
        ],
        temperature=0.4,
        max_tokens=1800,   # ê¸°ì¡´ 1100 â†’ 1800 ìƒí–¥
        presence_penalty=0.0,
        frequency_penalty=0.1,
    ).choices[0].message.content.strip()

    # 5) ëŠê¹€ ë°©ì§€ ì´ì–´ì“°ê¸°(pass-2)
    if not draft.rstrip().endswith(("ë‹¤.", "ìš”.", ".", "!", "?")) or _missing_sections(draft):
        cont = client.chat.completions.create(
            model=gen_model,
            messages=[
                {"role":"system","content": SYSTEM_ROLE or "í•œêµ­ì–´ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€"},
                {"role":"user","content": f"ì•„ë˜ ë³´ê³ ì„œë¥¼ ì¤‘ë³µ ì—†ì´ ì´ì–´ì„œ ì™„ê²°ì„± ìˆê²Œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”. ëª¨ë“  ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\n{draft}"},
            ],
            temperature=0.3,
            max_tokens=700,
        ).choices[0].message.content.strip()
        draft = (draft + "\n\n" + cont).strip()

    # 6) ì„¹ì…˜ ëˆ„ë½ ë³´ì™„(pass-3)
    draft = _complete_missing_sections(client, gen_model, draft)

    # 7) ë ˆì´ì•„ì›ƒ/ë¬¸ì²´ ì •ë¦¬
    formatted = _refine_layout_markdown(client, gen_model, draft)
    polished = _refine_style_coherence(client, gen_model, formatted)

    # 8) í‰ë¬¸í™”
    plain = strip_markdown(polished)

    return {
        "report_text": plain,
        "emotion_trend": emotion_scores,
        "top_keywords": top_keywords,
    }
